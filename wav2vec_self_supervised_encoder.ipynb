{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRbzLhS_FCfw"
   },
   "source": [
    "wav2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FprCP1zBHa2j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torchaudio\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "from pathlib import Path\n",
    "\n",
    "audiofiles = get_all_files('../../data/speech/', match_and=['.wav'])\n",
    "print(len(audiofiles))\n",
    "\n",
    "def split_data(audiofiles):\n",
    "    \n",
    "    train_valid_files, test_files = [], []\n",
    "  \n",
    "    for audiofile in audiofiles:\n",
    "\n",
    "        if 'FR01' in audiofile:\n",
    "            if 'H03' in audiofile:\n",
    "                test_files.append(audiofile)\n",
    "            elif 'F03' in audiofile:\n",
    "                test_files.append(audiofile)\n",
    "            else:\n",
    "                train_valid_files.append(audiofile)\n",
    "\n",
    "        elif 'FR02' in audiofile:\n",
    "            if 'H02' in audiofile:\n",
    "                test_files.append(audiofile)\n",
    "            elif 'F02' in audiofile:\n",
    "                test_files.append(audiofile)\n",
    "            else:\n",
    "                train_valid_files.append(audiofile)\n",
    "\n",
    "        elif 'GB' in audiofile:\n",
    "            if 'H02' in audiofile:\n",
    "                test_files.append(audiofile)\n",
    "            elif 'F02' in audiofile:\n",
    "                test_files.append(audiofile) \n",
    "            else:\n",
    "                train_valid_files.append(audiofile)\n",
    "  \n",
    "    return train_valid_files, test_files\n",
    "\n",
    "# Splitting data\n",
    "train_valid_files, test_files = split_data(audiofiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JmbDMZbgQ_2E"
   },
   "outputs": [],
   "source": [
    "def create_json(json_file, audiolist):\n",
    "    json_dict = {}\n",
    "    for audiofile in audiolist:\n",
    "      # Getting info\n",
    "      audioinfo = torchaudio.info(audiofile)\n",
    "\n",
    "      # Compute the duration in seconds.\n",
    "      # This is the number of samples divided by the sampling frequency\n",
    "      duration =  round(audioinfo.num_frames/audioinfo.sample_rate, 2)\n",
    "\n",
    "      # Get digit Label by manipulating the audio path\n",
    "      level =  audiofile.split('/')[-2]\n",
    "\n",
    "      # Get a unique utterance id\n",
    "      uttid =  audiofile.split('.wav')[0].split('/')[-1]\n",
    "      \n",
    "      # Get gender\n",
    "      if uttid[0] == 'F':\n",
    "          gender = \"female\"\n",
    "      else:\n",
    "          gender = \"male\"\n",
    "\n",
    "      # Create entry for this utterance\n",
    "      json_dict[uttid] = {\n",
    "              \"path\": audiofile,\n",
    "              \"length\": duration,\n",
    "              \"level\": level,\n",
    "              \"gender\": gender\n",
    "      }\n",
    "\n",
    "      # Writing the dictionary to the json file\n",
    "    with open(json_file, mode=\"w\") as json_f:\n",
    "      json.dump(json_dict, json_f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5mXoSImwRLX8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "create_json('train_valid.json', train_valid_files)\n",
    "create_json('test.json', test_files)   \n",
    "\n",
    "print(len(train_valid_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "k6GbwVMvRYDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hparams_wav2vec.yaml\n"
     ]
    }
   ],
   "source": [
    "%%file hparams_wav2vec.yaml\n",
    "\n",
    "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
    "seed: 1993\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "\n",
    "# URL for the wav2vec2 model, you can change to benchmark diffrenet models\n",
    "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
    "# This allow you to have ~4% improvment\n",
    "sslmodel_hub: facebook/wav2vec2-base\n",
    "sslmodel_folder: ./results/Level_Gender_Classification/wav2vec/1986/save\n",
    "\n",
    "# Path where data manifest files are stored\n",
    "train_valid_annotation: train_valid.json\n",
    "test_annotation: test.json\n",
    "data_augmentation: False\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "number_of_epochs: 5\n",
    "batch_size: 16\n",
    "lr: 0.0001\n",
    "lr_sslmodel: 0.00001\n",
    "sample_rate: 16000\n",
    "\n",
    "#freeze all wav2vec2\n",
    "freeze_sslmodel: False\n",
    "# freeze_sslmodel: True\n",
    "#set to true to freeze the CONV part of the wav2vec2 model\n",
    "# We see an improvement of 2% with freezing CNNs\n",
    "freeze_sslmodel_conv: True\n",
    "\n",
    "# Model parameters\n",
    "encoder_dim: 768\n",
    "\n",
    "n_classes: 3 # In this case, we have 3 levels\n",
    "n_gender_classes: 2 # male and female\n",
    "\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>\n",
    "    shuffle: True\n",
    "    num_workers: 2  # 2 on linux but 0 works on windows\n",
    "    drop_last: False\n",
    "\n",
    "# Wav2vec2 encoder\n",
    "sslmodel: !new:speechbrain.lobes.models.huggingface_wav2vec.HuggingFaceWav2Vec2\n",
    "    source: !ref <sslmodel_hub>\n",
    "    output_norm: True\n",
    "    freeze: !ref <freeze_sslmodel>\n",
    "    freeze_feature_extractor: !ref <freeze_sslmodel_conv>\n",
    "    save_path: !ref <sslmodel_folder>\n",
    "\n",
    "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
    "    return_std: False\n",
    "\n",
    "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <encoder_dim>\n",
    "    n_neurons: !ref <n_classes>\n",
    "    bias: False\n",
    "\n",
    "gender_output_mlp: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <encoder_dim>\n",
    "    n_neurons: !ref <n_gender_classes>\n",
    "    bias: False\n",
    "\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs>\n",
    "\n",
    "modules:\n",
    "    sslmodel: !ref <sslmodel>\n",
    "    output_mlp: !ref <output_mlp>\n",
    "    gender_output_mlp: !ref <gender_output_mlp>\n",
    "\n",
    "model: !new:torch.nn.ModuleList\n",
    "    - [!ref <output_mlp>]\n",
    "\n",
    "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
    "    apply_log: True\n",
    "\n",
    "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
    "\n",
    "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
    "    metric: !name:speechbrain.nnet.losses.classification_error\n",
    "        reduction: batch\n",
    "            \n",
    "gender_error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
    "    metric: !name:speechbrain.nnet.losses.classification_error\n",
    "        reduction: batch\n",
    "       \n",
    "opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr>\n",
    "\n",
    "sslmodel_opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr_sslmodel>\n",
    "\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.9\n",
    "    patient: 0\n",
    "\n",
    "lr_annealing_sslmodel: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr_sslmodel>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.9\n",
    "\n",
    "recoverables:\n",
    "    model: !ref <model>\n",
    "    sslmodel: !ref <sslmodel>\n",
    "    lr_annealing_output: !ref <lr_annealing>\n",
    "    lr_annealing_sslmodel: !ref <lr_annealing_sslmodel>\n",
    "    counter: !ref <epoch_counter>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPRcYKd6R_Ok",
    "outputId": "ce95c0e2-d322-4331-ce1d-db44479fd86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%file train.py\n",
    "\n",
    "\"Recipe for training a level_gender classification system.\"\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Brain class for speech enhancement training\n",
    "class DigitBrain(sb.Brain):\n",
    "    \"\"\"Class that manages the training loop. See speechbrain.core.Brain.\"\"\"\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Runs all the computations that transforms the input into the\n",
    "        output probabilities over the N classes.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Tensor\n",
    "            Tensor that contains the posterior probabilities over the N classes.\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, lens = batch.sig\n",
    "        \n",
    "        outputs = self.modules.sslmodel(wavs, lens)\n",
    "\n",
    "        # last dim will be used for AdaptativeAVG pool\n",
    "        outputs = self.hparams.avg_pool(outputs, lens)\n",
    "        outputs = outputs.view(outputs.shape[0], -1)\n",
    "\n",
    "        level_logits = self.modules.output_mlp(outputs)\n",
    "        gender_logits = self.modules.gender_output_mlp(outputs)\n",
    "\n",
    "        level_predictions = self.hparams.log_softmax(level_logits)\n",
    "        gender_predictions = self.hparams.log_softmax(gender_logits)\n",
    "        \n",
    "        return level_predictions, gender_predictions\n",
    "\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : tensor\n",
    "            The output tensor from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        levelId, _ = batch.level_encoded\n",
    "        genderId, _ = batch.gender_encoded\n",
    "\n",
    "        levelId = levelId.squeeze(1)\n",
    "        genderId = genderId.squeeze(1)\n",
    "        \n",
    "        level_predictions, gender_predictions = predictions\n",
    "        \n",
    "        level_loss = self.hparams.compute_cost(level_predictions, levelId)\n",
    "        gender_loss = self.hparams.compute_cost(gender_predictions, genderId)\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, level_predictions, levelId)\n",
    "            self.gender_error_metrics.append(batch.id, gender_predictions, genderId)\n",
    "        \n",
    "        loss = gender_loss + (3*level_loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Trains the parameters given a single batch in input\"\"\"\n",
    "\n",
    "        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "        loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)\n",
    "        loss.backward()\n",
    "        if self.check_gradients(loss):\n",
    "            self.sslmodel_optimizer.step()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.sslmodel_optimizer.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\"Gets called at the beginning of each epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up statistics trackers for this stage\n",
    "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=sb.nnet.losses.nll_loss\n",
    "        )\n",
    "        self.gender_loss_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=sb.nnet.losses.nll_loss\n",
    "        )\n",
    "\n",
    "        # Set up evaluation-only statistics trackers\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics = self.hparams.error_stats()\n",
    "            self.gender_error_metrics = self.hparams.gender_error_stats()\n",
    "\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\"Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage.\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "\n",
    "        # Summarize the statistics from the stage for record-keeping.\n",
    "        else:\n",
    "            stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"error_rate\": self.error_metrics.summarize(\"average\"),\n",
    "                \"gender_error\": self.gender_error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # At the end of validation...\n",
    "        if stage == sb.Stage.VALID:\n",
    "\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(stats[\"error_rate\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "\n",
    "            (\n",
    "                old_lr_sslmodel,\n",
    "                new_lr_sslmodel,\n",
    "            ) = self.hparams.lr_annealing_sslmodel(stats[\"error_rate\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.sslmodel_optimizer, new_lr_sslmodel\n",
    "            )\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch\": epoch, \"lr\": old_lr, \"sslmodel_lr\": old_lr_sslmodel},\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats=stats,\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints,\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta=stats, min_keys=[\"error_rate\"]\n",
    "            )\n",
    "\n",
    "        # We also write statistics about test data to stdout and to logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stats,\n",
    "            )\n",
    "\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the sslmodel optimizer and model optimizer\"\n",
    "        self.sslmodel_optimizer = self.hparams.sslmodel_opt_class(\n",
    "            self.modules.sslmodel.parameters()\n",
    "        )\n",
    "        self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"sslmodel_opt\", self.sslmodel_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        self.sslmodel_optimizer.zero_grad(set_to_none)\n",
    "        self.optimizer.zero_grad(set_to_none)\n",
    "\n",
    "def dataio_prep(hparams, phase):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "    We expect `prepare_mini_librispeech` to have been called before this,\n",
    "    so that the `train.json`, `valid.json`,  and `valid.json` manifest files\n",
    "    are available.\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : dict\n",
    "        Contains two keys, \"train\" and \"valid\" that correspond\n",
    "        to the appropriate DynamicItemDataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization of the label encoder. The label encoder assigns to each\n",
    "    # of the observed label a unique index (e.g, 'FR01': 0, 'FR01': 1, ..)\n",
    "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
    "    gender_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
    "\n",
    "    # Define audio pipeline\n",
    "    @sb.utils.data_pipeline.takes(\"path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline_subset(path):\n",
    "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
    "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig, fs = torchaudio.load(path)\n",
    "\n",
    "        # Resampling\n",
    "        sig = sig[-1]\n",
    "        sig = torchaudio.functional.resample(sig, fs, hparams[\"sample_rate\"])\n",
    "                \n",
    "        # Get the 8 second of the sig\n",
    "        maxNumber = sig.shape[-1] - (8 * 16000)\n",
    "        number = random.randint(0, maxNumber)\n",
    "        sig = sig[number:number + (8*16000)]\n",
    "        \n",
    "        return sig\n",
    "        \n",
    "    @sb.utils.data_pipeline.takes(\"path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(path):\n",
    "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
    "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig, fs = torchaudio.load(path)\n",
    "\n",
    "        # Resampling\n",
    "        sig = sig[-1]\n",
    "        sig = torchaudio.functional.resample(sig, fs, hparams[\"sample_rate\"])\n",
    "        \n",
    "        return sig\n",
    "\n",
    "    # Define label pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"level\")\n",
    "    @sb.utils.data_pipeline.provides(\"level\", \"level_encoded\")\n",
    "    def label_pipeline(level):\n",
    "        \"\"\"Defines the pipeline to process the digit labels.\n",
    "        Note that we have to assign a different integer to each class\n",
    "        through the label encoder.\n",
    "        \"\"\"\n",
    "        yield level\n",
    "        level_encoded = label_encoder.encode_label_torch(level)\n",
    "        yield level_encoded\n",
    "        \n",
    "    @sb.utils.data_pipeline.takes(\"gender\")\n",
    "    @sb.utils.data_pipeline.provides(\"gender\", \"gender_encoded\")\n",
    "    def gender_pipeline(gender):\n",
    "        \"\"\"Defines the pipeline to process the digit labels.\n",
    "        Note that we have to assign a different integer to each class\n",
    "        through the label encoder.\n",
    "        \"\"\"\n",
    "        yield gender\n",
    "        gender_encoded = gender_encoder.encode_label_torch(gender)\n",
    "        yield gender_encoded\n",
    "        \n",
    "\n",
    "    # Define datasets. We also connect the dataset with the data processing\n",
    "    # functions defined above.\n",
    "    datasets = {}\n",
    "    if phase == \"train\":\n",
    "        data_info = {\n",
    "            \"train\": hparams[\"train_annotation\"],\n",
    "            \"valid\": hparams[\"valid_annotation\"],\n",
    "        }\n",
    "        hparams[\"dataloader_options\"][\"shuffle\"] = True\n",
    "        for dataset in data_info:\n",
    "            datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "                json_path=data_info[dataset],\n",
    "                dynamic_items=[audio_pipeline_subset, label_pipeline, gender_pipeline],\n",
    "                output_keys=[\"id\", \"sig\", \"level_encoded\", \"gender_encoded\"],\n",
    "            )\n",
    "            \n",
    "        # Load or compute the label encoder (with multi-GPU DDP support)\n",
    "        # Please, take a look into the lab_enc_file to see the label to index\n",
    "        # mapping.\n",
    "        lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "        label_encoder.load_or_create(\n",
    "            path=lab_enc_file,\n",
    "            from_didatasets=[datasets[\"train\"]],\n",
    "            output_key=\"level\",\n",
    "        )\n",
    "\n",
    "        lab_enc_file = os.path.join(hparams[\"save_folder\"], \"gender_encoder.txt\")\n",
    "        gender_encoder.load_or_create(\n",
    "            path=lab_enc_file,\n",
    "            from_didatasets=[datasets[\"train\"]],\n",
    "            output_key=\"gender\",\n",
    "        )\n",
    "    else:\n",
    "        data_info = {\n",
    "            \"test\": hparams[\"test_annotation\"],\n",
    "            \"train_valid\": hparams[\"train_valid_annotation\"],\n",
    "        }\n",
    "        hparams[\"dataloader_options\"][\"shuffle\"] = True\n",
    "        for dataset in data_info:\n",
    "            datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "                json_path=data_info[dataset],\n",
    "                dynamic_items=[audio_pipeline_subset, label_pipeline, gender_pipeline],\n",
    "                output_keys=[\"id\", \"sig\", \"level_encoded\", \"gender_encoded\"],\n",
    "            )        \n",
    "        # Load or compute the label encoder (with multi-GPU DDP support)\n",
    "        # Please, take a look into the lab_enc_file to see the label to index\n",
    "        # mapping.\n",
    "        lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "        label_encoder.load_or_create(\n",
    "            path=lab_enc_file,\n",
    "            from_didatasets=[datasets[\"train_valid\"]],\n",
    "            output_key=\"level\",\n",
    "        )\n",
    "\n",
    "        lab_enc_file = os.path.join(hparams[\"save_folder\"], \"gender_encoder.txt\")\n",
    "        gender_encoder.load_or_create(\n",
    "            path=lab_enc_file,\n",
    "            from_didatasets=[datasets[\"train_valid\"]],\n",
    "            output_key=\"gender\",\n",
    "        )\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Recipe begins!\n",
    "if __name__ == \"__main__\":\n",
    "      \n",
    "    # Reading command line arguments.\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "        \n",
    "    # Reading the train-valid data drom json\n",
    "    id_list = []\n",
    "    with open('train_valid.json', 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "        # Access the data\n",
    "        for key in json_data:\n",
    "            if 'H' in key:\n",
    "                if 'FR1' in key:\n",
    "                    item = [key, 'FR1', 'Male']\n",
    "                elif 'FR2' in key:\n",
    "                    item = [key, 'FR2', 'Male']\n",
    "                else:\n",
    "                    item = [key, 'GB', 'Male']\n",
    "            else:\n",
    "                if 'FR1' in key:\n",
    "                    item = [key, 'FR1', 'Female']\n",
    "                elif 'FR2' in key:\n",
    "                    item = [key, 'FR2', 'Female']\n",
    "                else:\n",
    "                    item = [key, 'GB', 'Female']    \n",
    "            id_list.append(item)\n",
    "    \n",
    "    # Create the pandas DataFrame\n",
    "    df = pd.DataFrame(id_list, columns=['ID', 'level', 'gender'])\n",
    "    df['level_gender'] = df['level'].astype(str) + '_' + df['gender'].astype(str)\n",
    "  \n",
    "    # Create StratifiedKFold on level_gender\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    target = df.loc[:,'level_gender']\n",
    "    \n",
    "    fold_no = 1\n",
    "    min_test_stats = 100\n",
    "    for train_index, valid_index in skf.split(df, target):\n",
    "\n",
    "        # Load hyperparameters file with command-line overrides.\n",
    "        with open(hparams_file) as fin:\n",
    "            hparams = load_hyperpyyaml(fin,  overrides)\n",
    "        \n",
    "        # Creating output_folder/save_folder/train_log/checkpointer/train_logger for each fold\n",
    "        hparams['output_folder'] = './results/Level_Gender_Classification/Xvector/1986/Fold' + str(fold_no)\n",
    "        hparams['save_folder'] = hparams['output_folder'] + \"/save\"\n",
    "        hparams['train_log'] = hparams['output_folder'] + \"/train_log.txt\"\n",
    "        hparams['checkpointer'] = sb.utils.checkpoints.Checkpointer(checkpoints_dir = hparams['save_folder'], recoverables = hparams['recoverables'])\n",
    "        hparams['train_logger'] = sb.utils.train_logger.FileTrainLogger(save_file = hparams['train_log'])\n",
    "        hparams['train_annotation'] = hparams['output_folder'] + '/train.json'\n",
    "        hparams['valid_annotation'] = hparams['output_folder'] + '/valid.json'\n",
    "        hparams['sslmodel_folder'] = hparams['save_folder'] + '/ssl_checkpoint'\n",
    "\n",
    "        train = df.loc[train_index,:]\n",
    "        valid = df.loc[valid_index,:]\n",
    "\n",
    "        # Create train keys\n",
    "        train_keys = []\n",
    "        train_ids = df['ID'].iloc[train_index]\n",
    "        for train_id in train_ids:\n",
    "            train_keys.append(train_id)\n",
    "        \n",
    "        # Create valid keys\n",
    "        valid_keys = []\n",
    "        val_ids = df['ID'].iloc[valid_index]\n",
    "        for val_id in val_ids:\n",
    "            valid_keys.append(val_id)\n",
    "        \n",
    "        # Filter the data based on a condition\n",
    "        train_filtered_data = {key: json_data[key] for key in json_data if key in train_keys}\n",
    "        valid_filtered_data = {key: json_data[key] for key in json_data if key in valid_keys}\n",
    "\n",
    "        # Convert the filtered data to JSON format\n",
    "        train_json_output = json.dumps(train_filtered_data)\n",
    "        train_dict = json.loads(train_json_output)\n",
    "                \n",
    "        # Convert the filtered data to JSON format\n",
    "        valid_json_output = json.dumps(valid_filtered_data)\n",
    "        valid_dict = json.loads(valid_json_output)\n",
    "        \n",
    "        # Create json for validation set\n",
    "        with open(hparams['output_folder'] + '/train.json', 'w') as f:\n",
    "            json.dump(train_dict, f, indent=2)   \n",
    "            \n",
    "        # Create json for validation set\n",
    "        with open(hparams['output_folder'] + '/valid.json', 'w') as f:\n",
    "            json.dump(valid_dict, f, indent=2)\n",
    "\n",
    "        # Start print\n",
    "        print('--------------------------------')\n",
    "        print(f'FOLD {fold_no}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        datasets = dataio_prep(hparams, \"train\")\n",
    "        \n",
    "        # Create experiment directory\n",
    "        sb.create_experiment_directory(\n",
    "            experiment_directory=hparams[\"output_folder\"],\n",
    "            hyperparams_to_save=hparams_file,\n",
    "            overrides=overrides,\n",
    "        )\n",
    "\n",
    "        hparams[\"sslmodel\"] = hparams[\"sslmodel\"].to(device=run_opts[\"device\"])\n",
    "        # freeze the feature extractor part when unfreezing\n",
    "        if not hparams[\"freeze_sslmodel\"] and hparams[\"freeze_sslmodel_conv\"]:\n",
    "            hparams[\"sslmodel\"].model.feature_extractor._freeze_parameters()\n",
    "\n",
    "        # Initialize the Brain object to prepare for mask training.\n",
    "        digit_brain = DigitBrain(\n",
    "            modules=hparams[\"modules\"],\n",
    "            opt_class=hparams[\"opt_class\"],\n",
    "            hparams=hparams,\n",
    "            run_opts=run_opts,\n",
    "            checkpointer=hparams[\"checkpointer\"],\n",
    "        )\n",
    "\n",
    "        # The `fit()` method iterates the training loop, calling the methods\n",
    "        # necessary to update the parameters of the model. Since all objects\n",
    "        # with changing state are managed by the Checkpointer, training can be\n",
    "        # stopped at any point, and will be resumed on next call.\n",
    "        digit_brain.fit(\n",
    "            epoch_counter=digit_brain.hparams.epoch_counter,\n",
    "            train_set=datasets[\"train\"],\n",
    "            valid_set=datasets[\"valid\"],\n",
    "            train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "            valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "        )\n",
    "\n",
    "        #Load the best checkpoint for evaluation\n",
    "        test_stats = digit_brain.evaluate(\n",
    "            test_set=datasets[\"valid\"],\n",
    "            min_key=\"error\",\n",
    "            test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "        )\n",
    "        \n",
    "        # Getting the best fold base on test_stats\n",
    "        if test_stats < min_test_stats: \n",
    "            min_test_stats = test_stats\n",
    "            best_model = digit_brain\n",
    "            best_fold = fold_no\n",
    "        \n",
    "        fold_no += 1\n",
    "        \n",
    "    # Train the model with train+valid data and evaluate it by test data \n",
    "    # Load hyperparameters file with command-line overrides.\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin,  overrides)\n",
    "    \n",
    "    # Creating output_folder/save_folder/train_log/checkpointer/train_logger for each fold\n",
    "    hparams['output_folder'] = './results/Level_Gender_Classification/wav2vec/1986/FinalFold'\n",
    "    hparams['save_folder'] = hparams['output_folder'] + \"/save\"\n",
    "    hparams['train_log'] = hparams['output_folder'] + \"/train_log.txt\"\n",
    "    hparams['checkpointer'] = sb.utils.checkpoints.Checkpointer(checkpoints_dir = hparams['save_folder'], recoverables = hparams['recoverables'])\n",
    "    hparams['train_logger'] = sb.utils.train_logger.FileTrainLogger(save_file = hparams['train_log'])\n",
    "\n",
    "    # Start print\n",
    "    print('--------------------------------')\n",
    "    print(f'Final Result')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    datasets = dataio_prep(hparams, \"test\")\n",
    "    \n",
    "    # Create experiment directory\n",
    "    sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "\n",
    "    hparams[\"sslmodel\"] = hparams[\"sslmodel\"].to(device=run_opts[\"device\"])\n",
    "    # freeze the feature extractor part when unfreezing\n",
    "    if not hparams[\"freeze_sslmodel\"] and hparams[\"freeze_sslmodel_conv\"]:\n",
    "        hparams[\"sslmodel\"].model.feature_extractor._freeze_parameters()\n",
    "\n",
    "    # Initialize the Brain object to prepare for mask training.\n",
    "    digit_brain = DigitBrain(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "    # The `fit()` method iterates the training loop, calling the methods\n",
    "    # necessary to update the parameters of the model. Since all objects\n",
    "    # with changing state are managed by the Checkpointer, training can be\n",
    "    # stopped at any point, and will be resumed on next call.\n",
    "    digit_brain.fit(\n",
    "        epoch_counter=digit_brain.hparams.epoch_counter,\n",
    "        train_set=datasets[\"train_valid\"],\n",
    "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "\n",
    "    #Load the best checkpoint for evaluation\n",
    "    test_stats = digit_brain.evaluate(\n",
    "        test_set=datasets[\"test\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "aLmBKJd8nqHi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "Downloading (…)rocessor_config.json: 100%|█████| 159/159 [00:00<00:00, 13.7kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██| 1.84k/1.84k [00:00<00:00, 161kB/s]\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|█████████| 380M/380M [00:04<00:00, 92.2MB/s]\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold1\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold1/save/CKPT+2023-04-15+23-47-13+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 2.38e-01, test error_rate: 0.00e+00, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold2\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold2/save/CKPT+2023-04-15+23-48-35+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.32it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.53, test error_rate: 1.29e-01, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold3\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold3/save/CKPT+2023-04-15+23-49-58+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.29it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 7.93e-01, test error_rate: 9.68e-02, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold4\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold4/save/CKPT+2023-04-15+23-51-19+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.69it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 9.68e-02, test error_rate: 0.00e+00, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold5\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold5/save/CKPT+2023-04-15+23-52-48+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.25it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.39, test error_rate: 1.67e-01, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 6\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold6\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold6/save/CKPT+2023-04-15+23-54-15+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.32it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 4.91e-01, test error_rate: 3.33e-02, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 7\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold7\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold7/save/CKPT+2023-04-15+23-55-37+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.63it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.58e-01, test error_rate: 0.00e+00, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 8\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold8\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold8/save/CKPT+2023-04-15+23-57-03+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.20it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 4.58e-01, test error_rate: 6.67e-02, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 9\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold9\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold9/save/CKPT+2023-04-15+23-58-29+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.71it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 2.77e-01, test error_rate: 3.33e-02, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "FOLD 10\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold10\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold10/save/CKPT+2023-04-15+23-59-53+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 7.22e-01, test error_rate: 1.00e-01, test gender_error: 0.00e+00\n",
      "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
      "--------------------------------\n",
      "Final Result\n",
      "--------------------------------\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/wav2vec/1986/FinalFold\n",
      "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "speechbrain.utils.epoch_loop - Going into epoch 1\n",
      "100%|██████████████████████████| 19/19 [00:14<00:00,  1.34it/s, train_loss=3.93]\n",
      "speechbrain.utils.epoch_loop - Going into epoch 2\n",
      "100%|███████████████████████████| 19/19 [00:14<00:00,  1.35it/s, train_loss=3.1]\n",
      "speechbrain.utils.epoch_loop - Going into epoch 3\n",
      "100%|██████████████████████████| 19/19 [00:14<00:00,  1.34it/s, train_loss=1.54]\n",
      "speechbrain.utils.epoch_loop - Going into epoch 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 19/19 [00:14<00:00,  1.35it/s, train_loss=0.731]\n",
      "speechbrain.utils.epoch_loop - Going into epoch 5\n",
      "100%|█████████████████████████| 19/19 [00:14<00:00,  1.35it/s, train_loss=0.954]\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.20it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 2.05, test error_rate: 2.00e-01, test gender_error: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./results/Level_Gender_Classification/wav2vec/1986\n",
    "\n",
    "!python train.py hparams_wav2vec.yaml --device='cuda:0' --data_augmentation=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gender_loss + level_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision is not available - cannot save figures\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 1\n",
    "--------------------------------\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold1\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold1/save/CKPT+2023-04-15+23-47-13+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.60s/it]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 9.80e-02, test error_rate: 0.00e+00, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 2\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold2\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold2/save/CKPT+2023-04-15+23-48-35+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.30it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 5.19e-01, test error_rate: 1.29e-01, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 3\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold3\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold3/save/CKPT+2023-04-15+23-49-58+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.29it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 2.92e-01, test error_rate: 9.68e-02, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 4\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold4\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold4/save/CKPT+2023-04-15+23-51-19+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.71it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 5.28e-02, test error_rate: 0.00e+00, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 5\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold5\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold5/save/CKPT+2023-04-15+23-52-48+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.20it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 4.74e-01, test error_rate: 1.67e-01, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 6\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold6\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold6/save/CKPT+2023-04-15+23-54-15+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.30it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.83e-01, test error_rate: 3.33e-02, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 7\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold7\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold7/save/CKPT+2023-04-15+23-55-37+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.67it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 7.07e-02, test error_rate: 0.00e+00, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 8\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold8\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold8/save/CKPT+2023-04-15+23-57-03+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.20it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.60e-01, test error_rate: 6.67e-02, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 9\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold9\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold9/save/CKPT+2023-04-15+23-58-29+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.71it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.14e-01, test error_rate: 3.33e-02, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "FOLD 10\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/Xvector/1986/Fold10\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Loading a checkpoint from results/Level_Gender_Classification/Xvector/1986/Fold10/save/CKPT+2023-04-15+23-59-53+00\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 2.54e-01, test error_rate: 1.00e-01, test gender_error: 0.00e+00\n",
    "/home/as03720/anaconda3/envs/workspace/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors']\n",
    "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "speechbrain.lobes.models.huggingface_wav2vec - speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 feature extractor is frozen.\n",
    "--------------------------------\n",
    "Final Result\n",
    "--------------------------------\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
    "speechbrain.core - Beginning experiment!\n",
    "speechbrain.core - Experiment folder: ./results/Level_Gender_Classification/wav2vec/1986/FinalFold\n",
    "speechbrain.core - 90.2M trainable parameters in DigitBrain\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "speechbrain.utils.epoch_loop - Going into epoch 1\n",
    "100%|██████████████████████████| 19/19 [00:14<00:00,  1.34it/s, train_loss=1.72]\n",
    "speechbrain.utils.epoch_loop - Going into epoch 2\n",
    "100%|███████████████████████████| 19/19 [00:14<00:00,  1.35it/s, train_loss=1.3]\n",
    "speechbrain.utils.epoch_loop - Going into epoch 3\n",
    "100%|█████████████████████████| 19/19 [00:14<00:00,  1.34it/s, train_loss=0.587]\n",
    "speechbrain.utils.epoch_loop - Going into epoch 4\n",
    "100%|█████████████████████████| 19/19 [00:14<00:00,  1.35it/s, train_loss=0.214]\n",
    "speechbrain.utils.epoch_loop - Going into epoch 5\n",
    "100%|█████████████████████████| 19/19 [00:14<00:00,  1.34it/s, train_loss=0.193]\n",
    "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
    "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.23it/s]\n",
    "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 8.40e-01, test error_rate: 2.67e-01, test gender_error: 0.00e+00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "wE3MkINXfIUa",
    "outputId": "b0a00193-13e8-40ff-ee4e-66cb24477f79"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-9684cfb83d91>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'results/Level_Gender_Classification/CNN/1986/Fold1/train_log.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-9684cfb83d91>\u001b[0m in \u001b[0;36mget_losses\u001b[0;34m(log_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/Level_Gender_Classification/CNN/1986/Fold1/train_log.txt'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_losses(log_file):\n",
    "    \"\"\"This function takes in input a path of a log-file and outputs the train and\n",
    "    valid losses in lists of float numbers\"\"\"\n",
    "\n",
    "    # Your code here. Aim for 9-10 lines\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            words = l.split()\n",
    "            for i, word in enumerate(words):\n",
    "                if word == \"train\":\n",
    "                    train_losses.append(float(words[i + 2]))\n",
    "                    valid_losses.append(float(words[i + 6].split(',')[0]))\n",
    "\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "log_file = 'results/Level_Gender_Classification/wav2vec/1986/Fold1/train_log.txt'\n",
    "train_losses, valid_losses = get_losses(log_file)\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(valid_losses, label='valid')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
